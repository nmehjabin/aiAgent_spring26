{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Open Model + RAG vs. Large Model (GPT-4o Mini, No RAG)\n",
    "\n",
    "**Goal:** Determine whether a larger closed model (GPT-4o Mini) without any retrieval can match or beat a small open model (Qwen 2.5 1.5B) augmented with RAG.\n",
    "\n",
    "**Setup:**\n",
    "- Model: `gpt-4o-mini`\n",
    "- No tools, no retrieval \u2014 pure parametric knowledge only\n",
    "- Each question is sent as a fresh, independent API call (not a conversation)\n",
    "- Same query sets used in Exercise 1 (Model T manual + Congressional Record)\n",
    "\n",
    "**Questions to answer after running:**\n",
    "1. Does GPT-4o Mini do a better job than Qwen 2.5 1.5B at **avoiding hallucinations**?\n",
    "2. Which questions does GPT-4o Mini answer **correctly**?\n",
    "3. How does GPT-4o Mini's **pre-training cut-off date** relate to the age of each corpus?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Install & Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the OpenAI Python SDK if not already present\n",
    "try:\n",
    "    ip = get_ipython()\n",
    "    ip.run_line_magic('pip', 'install -q openai pandas')\n",
    "except NameError:\n",
    "    import subprocess, sys\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'openai', 'pandas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "\n",
    "# =============================================================================\n",
    "# SET YOUR OPENAI API KEY\n",
    "# =============================================================================\n",
    "# Option A \u2013 Colab secret (recommended: Secrets panel on the left sidebar)\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "    print(\"\u2714 API key loaded from Colab secrets\")\n",
    "except Exception:\n",
    "    # Option B \u2013 environment variable or hard-coded (not recommended for shared notebooks)\n",
    "    OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY', 'YOUR_KEY_HERE')\n",
    "    if OPENAI_API_KEY == 'YOUR_KEY_HERE':\n",
    "        print(\"\u26a0 Please set your OpenAI API key!\")\n",
    "    else:\n",
    "        print(\"\u2714 API key loaded from environment variable\")\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Model for this exercise\n",
    "GPT_MODEL = \"gpt-4o-mini\"\n",
    "print(f\"Model: {GPT_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Function: GPT-4o Mini, No RAG\n",
    "\n",
    "Each call is **independent** \u2014 no shared conversation history, no tools, no retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_direct_query(question: str, max_tokens: int = 512, temperature: float = 0.3) -> str:\n",
    "    \"\"\"\n",
    "    Send a single question to GPT-4o Mini with NO tools and NO retrieval context.\n",
    "    Each call is stateless \u2014 a fresh conversation every time.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    question    : The user question (string).\n",
    "    max_tokens  : Maximum tokens in the completion.\n",
    "    temperature : Sampling temperature (0 = deterministic, 1 = creative).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    The model's answer as a plain string.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=GPT_MODEL,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are a helpful assistant. \"\n",
    "                    \"Answer the user's question concisely using only your pre-trained knowledge. \"\n",
    "                    \"Do not use any tools or search the web.\"\n",
    "                )\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": question\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        # No 'tools' parameter \u2192 model has no tools available\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Sets (same as Exercise 1)\n",
    "\n",
    "The **Model T** questions test knowledge from a ~1919 service manual.  \n",
    "The **Congressional Record** questions test knowledge of very recent (Jan 2026) Congressional proceedings \u2014 well beyond GPT-4o Mini's training cut-off.\n",
    "\n",
    "The `note` field is **only for your evaluation** \u2014 it is never sent to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t_queries = [\n",
    "    {\n",
    "        \"question\": \"How do I adjust the carburetor on a Model T?\",\n",
    "        \"note\": \"Answer is in the Model T service manual (1919)\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the correct spark plug gap for a Model T Ford?\",\n",
    "        \"note\": \"Manual states ~7/16 inch (about the thickness of a smooth dime)\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How do I fix a slipping transmission band?\",\n",
    "        \"note\": \"Manual describes loosening lock-nut and adjusting screw, Cut No. 12\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What oil should I use in a Model T engine?\",\n",
    "        \"note\": \"Manual discusses oil level using pet cocks; does not specify modern viscosity grades\"\n",
    "    },\n",
    "]\n",
    "\n",
    "congressional_record_queries = [\n",
    "    {\n",
    "        \"question\": \"What did Mr. Flood have to say about Mayor David Black in Congress on January 13, 2026?\",\n",
    "        \"note\": \"From Congressional Record Jan 13, 2026 \u2014 after GPT-4o Mini training cut-off\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What mistake did Elise Stefanovic make in Congress on January 23, 2026?\",\n",
    "        \"note\": \"From Congressional Record Jan 23, 2026 \u2014 after GPT-4o Mini training cut-off\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the purpose of the Main Street Parity Act?\",\n",
    "        \"note\": \"Referenced in Congressional Record Jan 20, 2026 \u2014 may be partially after cut-off\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Who in Congress has spoken for and against funding of pregnancy centers?\",\n",
    "        \"note\": \"From Congressional Record Jan 21, 2026 \u2014 after GPT-4o Mini training cut-off\"\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Model T queries      : {len(model_t_queries)}\")\n",
    "print(f\"Congressional queries: {len(congressional_record_queries)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run GPT-4o Mini on All Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(queries: list, corpus_name: str, delay: float = 1.0) -> list:\n",
    "    \"\"\"\n",
    "    Run GPT-4o Mini on a list of query dicts, collect results,\n",
    "    and save a per-corpus CSV immediately after the last question.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    queries     : List of dicts with 'question' and 'note' keys.\n",
    "    corpus_name : Label for the corpus (e.g. 'MODEL_T').\n",
    "    delay       : Seconds to wait between API calls (avoids rate-limit errors).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List of result dicts ready for a DataFrame.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for i, item in enumerate(queries, start=1):\n",
    "        question = item[\"question\"]\n",
    "        note     = item.get(\"note\", \"\")\n",
    "\n",
    "        print(f\"\\n[{corpus_name}] Q{i}: {question}\")\n",
    "        print(\"-\" * 70)\n",
    "\n",
    "        try:\n",
    "            answer = gpt_direct_query(question)\n",
    "        except Exception as e:\n",
    "            answer = f\"ERROR: {e}\"\n",
    "\n",
    "        print(answer)\n",
    "\n",
    "        results.append({\n",
    "            \"corpus\"               : corpus_name,\n",
    "            \"question\"             : question,\n",
    "            \"gpt4o_mini_answer\"    : answer,\n",
    "            # --- Fill these in manually after reviewing the outputs ---\n",
    "            \"hallucinated\"         : None,   # True / False\n",
    "            \"factually_correct\"    : None,   # True / False / Partial\n",
    "            \"admits_uncertainty\"   : None,   # True / False\n",
    "            \"note_for_evaluator\"   : note,\n",
    "        })\n",
    "\n",
    "        # Small delay between calls to respect rate limits\n",
    "        if i < len(queries):\n",
    "            time.sleep(delay)\n",
    "\n",
    "    # \u2500\u2500 Save per-corpus CSV immediately after the last question \u2500\u2500\n",
    "    corpus_csv = f\"exercise2_{corpus_name.lower()}_results.csv\"\n",
    "    pd.DataFrame(results).to_csv(corpus_csv, index=False)\n",
    "    print(f\"\\n\u2714 Saved {len(results)} rows \u2192 {corpus_csv}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Run on Model T corpus ----\n",
    "print(\"=\" * 70)\n",
    "print(\"CORPUS: MODEL T SERVICE MANUAL (1919)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "model_t_results = run_experiment(model_t_queries, corpus_name=\"MODEL_T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Run on Congressional Record corpus ----\n",
    "print(\"=\" * 70)\n",
    "print(\"CORPUS: CONGRESSIONAL RECORD (Jan 2026)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "cr_results = run_experiment(congressional_record_queries, corpus_name=\"CONGRESSIONAL_RECORD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Results & Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Per-corpus CSVs were already saved by run_experiment() \u2500\u2500\n",
    "# exercise2_model_t_results.csv\n",
    "# exercise2_congressional_record_results.csv\n",
    "\n",
    "# \u2500\u2500 Merge into one combined CSV \u2500\u2500\n",
    "all_results = model_t_results + cr_results\n",
    "df = pd.DataFrame(all_results)\n",
    "\n",
    "csv_path = \"exercise2_gpt4o_mini_results.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"\u2714 Combined CSV saved: {csv_path}\")\n",
    "print(f\"  Rows: {len(df)}  |  Columns: {list(df.columns)}\\n\")\n",
    "\n",
    "# Display answers\n",
    "pd.set_option('display.max_colwidth', 120)\n",
    "display(df[['corpus', 'question', 'gpt4o_mini_answer']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Evaluation\n",
    "\n",
    "After reviewing the outputs above, fill in the evaluation columns in the cell below.\n",
    "\n",
    "| Column | Values | Meaning |\n",
    "|---|---|---|\n",
    "| `hallucinated` | True / False | Did the model invent specific facts not found in the source corpus? |\n",
    "| `factually_correct` | True / False / Partial | Is the answer accurate compared to the ground truth (manual or CR)? |\n",
    "| `admits_uncertainty` | True / False | Did the model say it wasn't sure or that the event may be outside its knowledge? |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# AUTO-EVALUATION: Upload your results CSV and let GPT-4o Mini judge each answer\n",
    "# =============================================================================\n",
    "# Upload your previously saved results CSV (e.g. exercise2_model_t_results.csv\n",
    "# or the combined exercise2_gpt4o_mini_results.csv). The cell will:\n",
    "#   1. Load the CSV\n",
    "#   2. Send each (question, answer) pair to GPT-4o Mini for evaluation\n",
    "#   3. Fill in hallucinated / factually_correct / admits_uncertainty automatically\n",
    "#   4. Save the evaluated CSV\n",
    "# =============================================================================\n",
    "\n",
    "import io, os\n",
    "\n",
    "# \u2500\u2500 Step 1: Load the results CSV \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# In Colab: triggers a file-upload dialog so you can pick your CSV.\n",
    "# Locally : set RESULTS_CSV_PATH to the path of your file.\n",
    "RESULTS_CSV_PATH = None  # e.g. \"exercise2_model_t_results.csv\"  \u2190 set for local use\n",
    "\n",
    "try:\n",
    "    from google.colab import files as colab_files\n",
    "    print(\"\ud83d\udcc2 Upload your results CSV (e.g. exercise2_model_t_results.csv):\")\n",
    "    uploaded = colab_files.upload()          # blocking dialog\n",
    "    csv_filename = list(uploaded.keys())[0]\n",
    "    df_eval = pd.read_csv(io.BytesIO(uploaded[csv_filename]))\n",
    "    print(f\"\u2714 Loaded '{csv_filename}'  ({len(df_eval)} rows)\")\n",
    "except (ImportError, Exception) as e:\n",
    "    if RESULTS_CSV_PATH and os.path.exists(RESULTS_CSV_PATH):\n",
    "        df_eval = pd.read_csv(RESULTS_CSV_PATH)\n",
    "        csv_filename = RESULTS_CSV_PATH\n",
    "        print(f\"\u2714 Loaded '{csv_filename}'  ({len(df_eval)} rows)\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\n",
    "            \"No file uploaded and RESULTS_CSV_PATH not set. \"\n",
    "            \"Set RESULTS_CSV_PATH to your CSV path and re-run.\"\n",
    "        ) from e\n",
    "\n",
    "print(\"Columns:\", list(df_eval.columns))\n",
    "display(df_eval[['corpus', 'question', 'gpt4o_mini_answer']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Step 2: Ground-truth hints for evaluation \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# These are the RAG answers from Exercise 1, used here as reference facts\n",
    "# to judge GPT-4o Mini's correctness. NOT sent during the main experiment.\n",
    "\n",
    "GROUND_TRUTH = {\n",
    "    \"How do I adjust the carburetor on a Model T?\": (\n",
    "        \"To adjust the carburetor on a Model T, refer to the instructions for your \"\n",
    "        \"specific model year. Generally, adjustments are made using the dashboard \"\n",
    "        \"adjustment mechanism, which controls the amount of fuel entering the carburetor \"\n",
    "        \"based on speed and temperature. For a cold start, turn the adjustment mechanism \"\n",
    "        \"slightly right to increase the air-fuel ratio, preventing knocking. In colder \"\n",
    "        \"temperatures, turn slightly left to reduce gasoline in the mixture. \"\n",
    "        \"Always consult the owner's manual for precise steps and safety guidelines.\"\n",
    "    ),\n",
    "\n",
    "    \"What is the correct spark plug gap for a Model T Ford?\": (\n",
    "        \"According to the context, the correct spark plug gap for a Model T Ford is \"\n",
    "        \"approximately 7/16 inch \u2014 about the thickness of a smooth dime. \"\n",
    "        \"The manual states: 'The distance between the sparking points should be 7/4', \"\n",
    "        \"referring to this measurement. All wire connections to spark plugs, coil box, \"\n",
    "        \"and commutator should at all times be kept in perfect contact.\"\n",
    "    ),\n",
    "\n",
    "    \"How do I fix a slipping transmission band?\": (\n",
    "        \"Loosen the lock nut at the tight side of the transmission cover, then adjust \"\n",
    "        \"the bands using the adjusting screws (refer to Cut No. 12). Ensure the bands \"\n",
    "        \"don't drag the drums when disengaging to prevent overheating the motor. \"\n",
    "        \"After making adjustments, reassemble the transmission cover. The manual does \"\n",
    "        \"not explicitly mention tightening the slow speed band directly \u2014 that step is \"\n",
    "        \"implied rather than stated outright. Actual repair procedures may vary, so \"\n",
    "        \"consulting an expert or official service manual is advisable.\"\n",
    "    ),\n",
    "\n",
    "    \"What oil should I use in a Model T engine?\": (\n",
    "        \"According to the context, the Model T manual states 'All other parts of the car \"\n",
    "        \"are properly oiled when it leaves the factory,' but does not specify a particular \"\n",
    "        \"oil type, brand, or viscosity grade. The manual only mentions maintaining the \"\n",
    "        \"correct oil level. Any standard automotive oil appropriate for the year and make \"\n",
    "        \"of the vehicle can be assumed, though specifics beyond what is stated are unknown.\"\n",
    "    ),\n",
    "\n",
    "    \"What did Mr. Flood have to say about Mayor David Black in Congress on January 13, 2026?\": (\n",
    "        \"In Congress on January 13, 2026, Mr. Flood recognized Mayor David Black from \"\n",
    "        \"Nebraska as a paragon of public service and a remarkable steward of the city's \"\n",
    "        \"affairs. He highlighted Black's contributions over nearly 17.5 years of service, \"\n",
    "        \"emphasizing how he guided the city into a thriving economic position. Mr. Flood \"\n",
    "        \"also noted Black's dedication to community engagement and outreach, and mentioned \"\n",
    "        \"that Black would not seek re-election, concluding an impressive career.\"\n",
    "    ),\n",
    "\n",
    "    \"What mistake did Elise Stefanovic make in Congress on January 23, 2026?\": (\n",
    "        \"According to the context, Elise Stefanovic tried to overturn the democratic \"\n",
    "        \"process by overruling repeated lawful orders from Capitol Police, forcibly entering \"\n",
    "        \"the Speaker's Lobby, attempting to break a window, and forcing her way through a \"\n",
    "        \"barricaded door while armed. This behavior was deemed inappropriate and illegal. \"\n",
    "        \"Key points: she ignored lawful orders from Capitol Police, pushed to the front of \"\n",
    "        \"the crowd disrupting orderly flow, broke a window, and attempted to force entry \"\n",
    "        \"while armed.\"\n",
    "    ),\n",
    "\n",
    "    \"What is the purpose of the Main Street Parity Act?\": (\n",
    "        \"The purpose of the Main Street Parity Act is to modify the criteria for loans \"\n",
    "        \"for plant acquisition, construction, conversion, or expansion by setting an equity \"\n",
    "        \"requirement at 10 percent, aligning these loans with the 504 standardization \"\n",
    "        \"programs. The act aims to provide small businesses with more equitable access to \"\n",
    "        \"capital while reflecting modern economic and technological realities.\"\n",
    "    ),\n",
    "\n",
    "    \"Who in Congress has spoken for and against funding of pregnancy centers?\": (\n",
    "        \"Ms. Dexter of Oregon spoke strongly against the funding of pregnancy centers, \"\n",
    "        \"while Mr. Schneider spoke in favor of them. Ms. Dexter stated that Democrats would \"\n",
    "        \"make claims about pregnancy resource centers, and Mr. Schneider argued that his \"\n",
    "        \"Republican colleagues brought the bill to the floor to funnel money to centers he \"\n",
    "        \"called anti-abortion. The Biden administration also made efforts to block states \"\n",
    "        \"from partnering with pregnancy centers.\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "def auto_evaluate(question: str, answer: str) -> dict:\n",
    "    \"\"\"\n",
    "    Ask GPT-4o Mini to evaluate a (question, answer) pair against ground truth.\n",
    "    Returns a dict with hallucinated, factually_correct, admits_uncertainty.\n",
    "    \"\"\"\n",
    "    ground_truth = GROUND_TRUTH.get(question, \"No ground truth available.\")\n",
    "\n",
    "    prompt = f\"\"\"You are an impartial evaluator. A model was asked a question and gave an answer.\n",
    "Compare the answer to the ground truth and return a JSON object with exactly these three fields:\n",
    "\n",
    "  hallucinated        : true if the answer states specific facts that are invented or not supported\n",
    "                        by the ground truth; false otherwise.\n",
    "  factually_correct   : \"True\" if fully correct, \"Partial\" if partly right, \"False\" if wrong.\n",
    "  admits_uncertainty  : true if the answer explicitly says it is unsure, doesn't know, or that\n",
    "                        the event may be outside its knowledge; false otherwise.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "GROUND TRUTH: {ground_truth}\n",
    "\n",
    "MODEL ANSWER: {answer}\n",
    "\n",
    "Respond with ONLY valid JSON. Example:\n",
    "{{\"hallucinated\": false, \"factually_correct\": \"Partial\", \"admits_uncertainty\": true}}\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=GPT_MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=100,\n",
    "        temperature=0,\n",
    "    )\n",
    "    raw = response.choices[0].message.content.strip()\n",
    "\n",
    "    import json as _json\n",
    "    try:\n",
    "        return _json.loads(raw)\n",
    "    except Exception:\n",
    "        return {\"hallucinated\": None, \"factually_correct\": raw, \"admits_uncertainty\": None}\n",
    "\n",
    "\n",
    "# \u2500\u2500 Step 3: Run auto-evaluation on every row \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "print(f\"Evaluating {len(df_eval)} answers with GPT-4o Mini...\\n\")\n",
    "\n",
    "for i, row in df_eval.iterrows():\n",
    "    result = auto_evaluate(row['question'], row['gpt4o_mini_answer'])\n",
    "    df_eval.at[i, 'hallucinated']       = result.get('hallucinated')\n",
    "    df_eval.at[i, 'factually_correct']  = result.get('factually_correct')\n",
    "    df_eval.at[i, 'admits_uncertainty'] = result.get('admits_uncertainty')\n",
    "\n",
    "    corpus  = row['corpus']\n",
    "    q_short = row['question'][:60]\n",
    "    print(f\"[{corpus}] {q_short}...\")\n",
    "    print(f\"  hallucinated={result.get('hallucinated')}  \"\n",
    "          f\"correct={result.get('factually_correct')}  \"\n",
    "          f\"admits_uncertainty={result.get('admits_uncertainty')}\")\n",
    "    time.sleep(0.5)\n",
    "\n",
    "# \u2500\u2500 Step 4: Save evaluated CSV \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "evaluated_csv = \"exercise2_evaluated_results.csv\"\n",
    "df_eval.to_csv(evaluated_csv, index=False)\n",
    "print(f\"\\n\u2714 Evaluated CSV saved: {evaluated_csv}\")\n",
    "\n",
    "# Also update the main df so the summary stats cell works\n",
    "df = df_eval.copy()\n",
    "csv_path = evaluated_csv\n",
    "\n",
    "display(df_eval[['corpus', 'question', 'hallucinated', 'factually_correct', 'admits_uncertainty']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run after filling in the evaluation columns above\n",
    "if df['hallucinated'].notna().any():\n",
    "    for corpus in df['corpus'].unique():\n",
    "        sub = df[df['corpus'] == corpus]\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Corpus: {corpus}  (n={len(sub)})\")\n",
    "        print(f\"{'='*50}\")\n",
    "        print(f\"  Hallucinated        : {sub['hallucinated'].sum()} / {len(sub)}\")\n",
    "        print(f\"  Factually correct   : {(sub['factually_correct'] == True).sum()} / {len(sub)}\")\n",
    "        print(f\"  Admits uncertainty  : {sub['admits_uncertainty'].sum()} / {len(sub)}\")\n",
    "else:\n",
    "    print(\"Fill in the evaluation columns first (cell above).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion Prompts\n",
    "\n",
    "Answer these in your write-up or as Markdown cells below.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Hallucination comparison: GPT-4o Mini vs Qwen 2.5 1.5B (no RAG)\n",
    "\n",
    "- For the **Model T** queries, which model hallucinated more confidently (gave specific but wrong facts)? Which was more likely to hedge?\n",
    "- For the **Congressional Record** queries, did GPT-4o Mini admit it couldn't know recent 2026 proceedings, or did it confabulate plausible-sounding congressional speech?\n",
    "- Overall: does a larger model without RAG outperform a smaller model without RAG on avoiding hallucinations?\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Which questions does GPT-4o Mini answer correctly?\n",
    "\n",
    "- List the question numbers it got **fully correct**, **partially correct**, and **wrong**.\n",
    "- Were the correctly-answered questions ones where the answer is **common general knowledge** (e.g., well-documented automotive trivia) vs. **corpus-specific details** (e.g., exact cut No. references in the manual)?\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Training cut-off vs. corpus age\n",
    "\n",
    "GPT-4o Mini's training data has a cut-off of roughly **October 2023** (OpenAI's stated cut-off for this model family).\n",
    "\n",
    "| Corpus | Date of material | Within cut-off? |\n",
    "|---|---|---|\n",
    "| Model T Service Manual | 1919 | \u2714 Yes \u2014 ~100 years old, well-documented online |\n",
    "| Congressional Record (used) | Jan 2026 | \u2717 No \u2014 ~2 years after cut-off |\n",
    "\n",
    "- For the **Model T** corpus: the manual is old and widely digitised. Does GPT-4o Mini show evidence of having absorbed this information into its weights? Where does it still err?\n",
    "- For the **Congressional Record** corpus: all four queries reference events in January 2026 \u2014 well after the cut-off. What is the expected failure mode? Did the model hallucinate, refuse, or correctly say it doesn't know?\n",
    "- **Conclusion:** In what scenario is GPT-4o Mini (no RAG) competitive with Qwen 2.5 1.5B + RAG? When does it clearly lose?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: download all CSVs in Colab\n",
    "try:\n",
    "    from google.colab import files\n",
    "    import os\n",
    "    for fname in [\n",
    "        \"exercise2_model_t_results.csv\",\n",
    "        \"exercise2_congressional_record_results.csv\",\n",
    "        csv_path,   # combined\n",
    "    ]:\n",
    "        if os.path.exists(fname):\n",
    "            files.download(fname)\n",
    "            print(f\"\u2b07 Downloading: {fname}\")\n",
    "        else:\n",
    "            print(f\"\u26a0 Not found (skipping): {fname}\")\n",
    "except ImportError:\n",
    "    print(\"Results saved locally:\")\n",
    "    for fname in [\n",
    "        \"exercise2_model_t_results.csv\",\n",
    "        \"exercise2_congressional_record_results.csv\",\n",
    "        csv_path,\n",
    "    ]:\n",
    "        print(f\"  {fname}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}