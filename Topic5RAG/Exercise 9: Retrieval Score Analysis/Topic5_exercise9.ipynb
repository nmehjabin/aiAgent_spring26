{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9: Retrieval Score Analysis\n",
    "\n",
    "**Goal:** Understand what similarity scores actually mean â€” when can you trust them, when are they ambiguous, and what threshold separates signal from noise?\n",
    "\n",
    "| Concept | Meaning |\n",
    "|---|---|\n",
    "| **Clear winner** | Large gap between score #1 and #2 â€” retriever is confident |\n",
    "| **Tight cluster** | All scores similar â€” retriever is uncertain which chunk is best |\n",
    "| **Score threshold** | Minimum score below which a chunk is considered irrelevant |\n",
    "\n",
    "**For each of 10 queries this notebook will:**\n",
    "1. Retrieve top-10 chunks and record all scores\n",
    "2. Compute gap, spread, and cluster statistics\n",
    "3. Plot score distributions\n",
    "4. Run RAG with and without a score threshold and compare answers\n",
    "5. Save everything to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ip = get_ipython()\n",
    "    ip.run_line_magic('pip', 'install -q torch transformers sentence-transformers faiss-cpu pymupdf accelerate ipyfilechooser pandas matplotlib')\n",
    "except NameError:\n",
    "    import subprocess, sys\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q',\n",
    "        'torch', 'transformers', 'sentence-transformers',\n",
    "        'faiss-cpu', 'pymupdf', 'accelerate', 'ipyfilechooser', 'pandas', 'matplotlib'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "import torch\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        name = torch.cuda.get_device_name(0)\n",
    "        mem  = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        print(f'âœ” CUDA GPU: {name} ({mem:.1f} GB)')\n",
    "        return 'cuda', torch.float16\n",
    "    elif torch.backends.mps.is_available():\n",
    "        print('âœ” Apple Silicon MPS')\n",
    "        return 'mps', torch.float32\n",
    "    else:\n",
    "        print('âš  CPU only')\n",
    "        return 'cpu', torch.float32\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    ENVIRONMENT = 'colab'\n",
    "except ImportError:\n",
    "    ENVIRONMENT = 'local'\n",
    "\n",
    "DEVICE, DTYPE = get_device()\n",
    "print(f'Environment: {ENVIRONMENT.upper()} | Device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 1 â€” SELECT DOCUMENT SOURCE  (DO NOT CHANGE)\n",
    "# =============================================================================\n",
    "USE_GOOGLE_DRIVE = True\n",
    "DOC_FOLDER = 'documents'\n",
    "folder_chooser = None\n",
    "\n",
    "if ENVIRONMENT == 'colab':\n",
    "    if USE_GOOGLE_DRIVE:\n",
    "        from google.colab import drive\n",
    "        print('Mounting Google Drive...')\n",
    "        drive.mount('/content/drive')\n",
    "        print('âœ” Google Drive mounted\\n')\n",
    "        try:\n",
    "            from ipyfilechooser import FileChooser\n",
    "            folder_chooser = FileChooser(\n",
    "                path='/content/drive/MyDrive',\n",
    "                title='Select your documents folder in Google Drive',\n",
    "                show_only_dirs=True, select_default=True)\n",
    "            print('ðŸ“ Select your documents folder below, then run Cell 2:')\n",
    "            display(folder_chooser)\n",
    "        except ImportError:\n",
    "            DOC_FOLDER = '/content/drive/MyDrive/your_documents_folder'\n",
    "            print(f\"Edit DOC_FOLDER: '{DOC_FOLDER}'\")\n",
    "    else:\n",
    "        from google.colab import files as colab_files\n",
    "        os.makedirs(DOC_FOLDER, exist_ok=True)\n",
    "        uploaded = colab_files.upload()\n",
    "        for fn in uploaded:\n",
    "            os.rename(fn, f'{DOC_FOLDER}/{fn}')\n",
    "else:\n",
    "    try:\n",
    "        from ipyfilechooser import FileChooser\n",
    "        folder_chooser = FileChooser(path=str(Path.home()),\n",
    "            title='Select documents folder', show_only_dirs=True, select_default=True)\n",
    "        display(folder_chooser)\n",
    "    except ImportError:\n",
    "        print(f'Using default folder: {DOC_FOLDER}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 2 â€” CONFIRM SELECTION  (DO NOT CHANGE)\n",
    "# =============================================================================\n",
    "if folder_chooser is not None and folder_chooser.selected_path:\n",
    "    DOC_FOLDER = folder_chooser.selected_path\n",
    "    print(f'âœ” Using: {DOC_FOLDER}')\n",
    "elif folder_chooser is not None:\n",
    "    print('âš  No folder selected â€” go back, select one, then re-run this cell.')\n",
    "else:\n",
    "    print(f'âœ” Using: {DOC_FOLDER}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(folder: str) -> List[Tuple[str, str]]:\n",
    "    docs = []\n",
    "    p = Path(folder)\n",
    "    if not p.exists():\n",
    "        print(f'âš  Folder not found: {folder}')\n",
    "        return docs\n",
    "    for path in sorted(p.iterdir()):\n",
    "        if path.suffix.lower() == '.pdf':\n",
    "            doc  = fitz.open(str(path))\n",
    "            text = ''.join(f'[Page {i+1}]\\n{page.get_text()}\\n' for i, page in enumerate(doc))\n",
    "            docs.append((path.name, text))\n",
    "        elif path.suffix.lower() in ('.txt', '.md'):\n",
    "            docs.append((path.name, path.read_text(encoding='utf-8', errors='replace')))\n",
    "    print(f'âœ” Loaded {len(docs)} documents')\n",
    "    for name, content in docs:\n",
    "        print(f'   {name}: {len(content):,} chars')\n",
    "    return docs\n",
    "\n",
    "documents = load_documents(DOC_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Build the RAG Pipeline\n",
    "Fixed chunk size (512) and overlap (128) â€” same as Exercise 1 defaults.\n",
    "Score analysis is the variable here, not chunk size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Chunk:\n",
    "    text: str\n",
    "    source_file: str\n",
    "    chunk_index: int\n",
    "    start_char: int\n",
    "    end_char: int\n",
    "\n",
    "\n",
    "def chunk_text(text: str, source_file: str,\n",
    "               chunk_size: int, chunk_overlap: int) -> list:\n",
    "    chunks, start, idx = [], 0, 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        if end < len(text):\n",
    "            pb = text.rfind('\\n\\n', start + chunk_size // 2, end)\n",
    "            if pb != -1:\n",
    "                end = pb + 2\n",
    "            else:\n",
    "                sb = text.rfind('. ', start + chunk_size // 2, end)\n",
    "                if sb != -1:\n",
    "                    end = sb + 2\n",
    "        s = text[start:end].strip()\n",
    "        if s:\n",
    "            chunks.append(Chunk(s, source_file, idx, start, end))\n",
    "            idx += 1\n",
    "        prev  = start\n",
    "        start = end - chunk_overlap\n",
    "        if start <= prev:\n",
    "            start = end\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def rebuild_pipeline(chunk_size: int = 512, chunk_overlap: int = 128):\n",
    "    \"\"\"Re-chunk documents, re-embed, rebuild FAISS index. Updates globals.\"\"\"\n",
    "    global all_chunks, index\n",
    "    all_chunks = []\n",
    "    for filename, content in documents:\n",
    "        all_chunks.extend(\n",
    "            chunk_text(content, filename,\n",
    "                       chunk_size=chunk_size,\n",
    "                       chunk_overlap=chunk_overlap)\n",
    "        )\n",
    "    chunk_embeddings = embed_model.encode(\n",
    "        [c.text for c in all_chunks],\n",
    "        show_progress_bar=True\n",
    "    ).astype('float32')\n",
    "    faiss.normalize_L2(chunk_embeddings)\n",
    "    index = faiss.IndexFlatIP(EMBEDDING_DIM)\n",
    "    index.add(chunk_embeddings)\n",
    "    print(f'Rebuilt: {len(all_chunks)} chunks, '\n",
    "          f'chunk_size={chunk_size}, chunk_overlap={chunk_overlap}')\n",
    "\n",
    "\n",
    "all_chunks = []\n",
    "index      = None\n",
    "print('âœ” Chunk + pipeline functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "EMBED_MODEL_NAME = 'all-MiniLM-L6-v2'\n",
    "EMBEDDING_DIM    = 384\n",
    "\n",
    "print(f'Loading embedding model: {EMBED_MODEL_NAME} ...')\n",
    "embed_model = SentenceTransformer(EMBED_MODEL_NAME, device=DEVICE)\n",
    "print('âœ” Embedding model ready')\n",
    "\n",
    "# Build index with Exercise 1 defaults\n",
    "CHUNK_SIZE    = 512\n",
    "CHUNK_OVERLAP = 128\n",
    "rebuild_pipeline(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "LLM_MODEL = 'Qwen/Qwen2.5-1.5B-Instruct'\n",
    "print(f'Loading LLM: {LLM_MODEL} ...')\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL)\n",
    "\n",
    "if DEVICE == 'cuda':\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        LLM_MODEL, device_map='auto', dtype=DTYPE, trust_remote_code=True)\n",
    "elif DEVICE == 'mps':\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        LLM_MODEL, dtype=DTYPE, trust_remote_code=True).to(DEVICE)\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        LLM_MODEL, dtype=DTYPE, trust_remote_code=True)\n",
    "\n",
    "print(f'âœ” LLM loaded on {DEVICE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Define 10 Queries\n",
    "Use a mix of types to see how score distributions differ by question difficulty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURE YOUR 10 QUERIES HERE\n",
    "# =============================================================================\n",
    "QUERIES = [\n",
    "    {'id': 'Q01', 'type': 'narrow_factual',\n",
    "     'text': 'What is the correct spark plug gap for a Model T Ford?'},\n",
    "\n",
    "    {'id': 'Q02', 'type': 'procedural',\n",
    "     'text': 'How do I fix a slipping transmission band?'},\n",
    "\n",
    "    {'id': 'Q03', 'type': 'procedural',\n",
    "     'text': 'How do I adjust the carburetor on a Model T?'},\n",
    "\n",
    "    {'id': 'Q04', 'type': 'broad_conceptual',\n",
    "     'text': 'What oil should I use in a Model T engine?'},\n",
    "\n",
    "    {'id': 'Q05', 'type': 'multi_section',\n",
    "     'text': 'What are all the steps to prepare a Model T for winter driving?'},\n",
    "\n",
    "    {'id': 'Q06', 'type': 'narrow_factual',\n",
    "     'text': 'How do I start a Model T Ford?'},\n",
    "\n",
    "    {'id': 'Q07', 'type': 'procedural',\n",
    "     'text': 'How do I check and fill the radiator on a Model T?'},\n",
    "\n",
    "    {'id': 'Q08', 'type': 'narrow_factual',\n",
    "     'text': 'What does the hand throttle lever do on a Model T?'},\n",
    "\n",
    "    {'id': 'Q09', 'type': 'off_topic',\n",
    "     'text': 'What is the latest iPhone model released by Apple?'},\n",
    "\n",
    "    {'id': 'Q10', 'type': 'ambiguous',\n",
    "     'text': 'How does the Ford engine work?'},\n",
    "]\n",
    "\n",
    "TOP_K = 10   # always retrieve top-10 for score analysis\n",
    "\n",
    "print(f'âœ” {len(QUERIES)} queries defined (TOP_K={TOP_K})')\n",
    "for q in QUERIES:\n",
    "    print(f\"   [{q['id']}] ({q['type']:<16}) {q['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Retrieve & Record All Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = (\n",
    "    'You are a helpful assistant. Answer the question using ONLY the context below.\\n'\n",
    "    'If the context does not contain enough information, say so.\\n\\n'\n",
    "    'CONTEXT:\\n{context}\\n\\n'\n",
    "    'QUESTION: {question}\\n\\n'\n",
    "    'ANSWER:'\n",
    ")\n",
    "\n",
    "\n",
    "def generate_response(prompt: str, max_new_tokens: int = 256) -> str:\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.1,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    return tokenizer.decode(\n",
    "        out[0][inputs['input_ids'].shape[1]:],\n",
    "        skip_special_tokens=True\n",
    "    ).strip()\n",
    "\n",
    "\n",
    "def retrieve_with_scores(question: str, top_k: int = 10) -> list:\n",
    "    \"\"\"Return list of (chunk, score) for top_k results.\"\"\"\n",
    "    q_emb = embed_model.encode([question]).astype('float32')\n",
    "    faiss.normalize_L2(q_emb)\n",
    "    scores, indices = index.search(q_emb, top_k)\n",
    "    return [\n",
    "        (all_chunks[i], float(s))\n",
    "        for s, i in zip(scores[0], indices[0]) if i != -1\n",
    "    ]\n",
    "\n",
    "\n",
    "def rag_with_threshold(question: str, retrieved: list,\n",
    "                        threshold: float = None) -> Tuple[str, int]:\n",
    "    \"\"\"\n",
    "    Generate an answer using only chunks that pass the score threshold.\n",
    "    Returns (answer, n_chunks_used).\n",
    "    \"\"\"\n",
    "    if threshold is not None:\n",
    "        kept = [(c, s) for c, s in retrieved if s >= threshold]\n",
    "    else:\n",
    "        kept = retrieved\n",
    "\n",
    "    if not kept:\n",
    "        return 'No chunks passed the score threshold.', 0\n",
    "\n",
    "    context = '\\n\\n---\\n\\n'.join(\n",
    "        f'[Source: {c.source_file} | Score: {s:.3f}]\\n{c.text}'\n",
    "        for c, s in kept\n",
    "    )\n",
    "    prompt = PROMPT_TEMPLATE.format(context=context, question=question)\n",
    "    return generate_response(prompt), len(kept)\n",
    "\n",
    "\n",
    "print('âœ” Retrieval and generation helpers defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SCORE THRESHOLD TO TEST\n",
    "# =============================================================================\n",
    "THRESHOLD = 0.4   # â† adjust after inspecting the score distributions below\n",
    "\n",
    "# â”€â”€ Run all 10 queries â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "score_rows   = []   # one row per (query Ã— rank) for detailed score CSV\n",
    "summary_rows = []   # one row per query for summary CSV\n",
    "\n",
    "for q in QUERIES:\n",
    "    print(f\"\\n[{q['id']}] ({q['type']}) {q['text']}\")\n",
    "\n",
    "    retrieved = retrieve_with_scores(q['text'], top_k=TOP_K)\n",
    "    scores    = [s for _, s in retrieved]\n",
    "\n",
    "    # â”€â”€ Score statistics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    top1_score    = scores[0] if scores else 0.0\n",
    "    top2_score    = scores[1] if len(scores) > 1 else 0.0\n",
    "    winner_gap    = top1_score - top2_score          # large = clear winner\n",
    "    score_spread  = scores[0] - scores[-1] if scores else 0.0\n",
    "    score_std     = float(np.std(scores))            # low = tight cluster\n",
    "    above_thresh  = sum(1 for s in scores if s >= THRESHOLD)\n",
    "\n",
    "    print(f'  top1={top1_score:.4f}  top2={top2_score:.4f}  '\n",
    "          f'gap={winner_gap:.4f}  std={score_std:.4f}  '\n",
    "          f'above_threshold({THRESHOLD})={above_thresh}/{TOP_K}')\n",
    "\n",
    "    # â”€â”€ Answers: no threshold vs. with threshold â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    answer_no_thresh, n_no_thresh   = rag_with_threshold(q['text'], retrieved, threshold=None)\n",
    "    answer_with_thresh, n_with_thresh = rag_with_threshold(q['text'], retrieved, threshold=THRESHOLD)\n",
    "\n",
    "    print(f'  No threshold  ({n_no_thresh} chunks): {answer_no_thresh[:120]}...')\n",
    "    print(f'  Thresholdâ‰¥{THRESHOLD} ({n_with_thresh} chunks): {answer_with_thresh[:120]}...')\n",
    "\n",
    "    # â”€â”€ Per-rank rows for detailed CSV â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    for rank, (chunk, score) in enumerate(retrieved, start=1):\n",
    "        score_rows.append({\n",
    "            'query_id'      : q['id'],\n",
    "            'query_type'    : q['type'],\n",
    "            'question'      : q['text'],\n",
    "            'rank'          : rank,\n",
    "            'score'         : round(score, 4),\n",
    "            'above_threshold': score >= THRESHOLD,\n",
    "            'source'        : chunk.source_file,\n",
    "            'char_len'      : len(chunk.text),\n",
    "            'chunk_preview' : chunk.text[:150].replace('\\n', ' '),\n",
    "        })\n",
    "\n",
    "    # â”€â”€ Summary row â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    summary_rows.append({\n",
    "        'query_id'           : q['id'],\n",
    "        'query_type'         : q['type'],\n",
    "        'question'           : q['text'],\n",
    "        'top1_score'         : round(top1_score, 4),\n",
    "        'top2_score'         : round(top2_score, 4),\n",
    "        'winner_gap'         : round(winner_gap, 4),\n",
    "        'score_spread'       : round(score_spread, 4),\n",
    "        'score_std'          : round(score_std, 4),\n",
    "        'n_above_threshold'  : above_thresh,\n",
    "        'threshold'          : THRESHOLD,\n",
    "        'answer_no_thresh'   : answer_no_thresh,\n",
    "        'answer_with_thresh' : answer_with_thresh,\n",
    "        'chunks_no_thresh'   : n_no_thresh,\n",
    "        'chunks_with_thresh' : n_with_thresh,\n",
    "    })\n",
    "\n",
    "# Save immediately\n",
    "df_scores  = pd.DataFrame(score_rows)\n",
    "df_summary = pd.DataFrame(summary_rows)\n",
    "df_scores .to_csv('exercise9_score_details.csv',  index=False)\n",
    "df_summary.to_csv('exercise9_score_summary.csv', index=False)\n",
    "print('\\nâœ” Saved: exercise9_score_details.csv  exercise9_score_summary.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Visualise Score Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Plot 1: Score profile per query (rank 1â€“10 on x-axis) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fig, axes = plt.subplots(2, 5, figsize=(18, 7), sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, q in zip(axes, QUERIES):\n",
    "    sub    = df_scores[df_scores['query_id'] == q['id']].sort_values('rank')\n",
    "    ranks  = sub['rank'].values\n",
    "    scores = sub['score'].values\n",
    "    colors = ['#e15759' if s >= THRESHOLD else '#aec7e8' for s in scores]\n",
    "\n",
    "    ax.bar(ranks, scores, color=colors)\n",
    "    ax.axhline(THRESHOLD, color='black', linestyle='--', linewidth=0.8,\n",
    "               label=f'threshold={THRESHOLD}')\n",
    "    ax.set_title(f\"{q['id']} ({q['type'][:10]})\", fontsize=8)\n",
    "    ax.set_xlabel('Rank', fontsize=7)\n",
    "    ax.set_ylabel('Score', fontsize=7)\n",
    "    ax.tick_params(labelsize=7)\n",
    "    ax.set_xticks(ranks)\n",
    "    ax.set_ylim(0, max(df_scores['score'].max() * 1.1, THRESHOLD * 1.2))\n",
    "\n",
    "    # Annotate top-1 score\n",
    "    ax.text(1, scores[0] + 0.01, f'{scores[0]:.3f}',\n",
    "            ha='center', va='bottom', fontsize=7, fontweight='bold')\n",
    "\n",
    "plt.suptitle(\n",
    "    f'Retrieval Score Profiles (top-{TOP_K} chunks per query)\\n'\n",
    "    f'Red = above threshold ({THRESHOLD}), Blue = below',\n",
    "    fontsize=11\n",
    ")\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.93])\n",
    "plt.savefig('exercise9_score_profiles.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('âœ” Saved: exercise9_score_profiles.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Plot 2: Winner gap vs. score spread â€” bubble chart â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "\n",
    "cmap   = plt.get_cmap('tab10')\n",
    "types  = df_summary['query_type'].unique()\n",
    "colour = {t: cmap(i) for i, t in enumerate(types)}\n",
    "\n",
    "for _, row in df_summary.iterrows():\n",
    "    ax.scatter(\n",
    "        row['winner_gap'], row['score_spread'],\n",
    "        s=row['top1_score'] * 800,\n",
    "        color=colour[row['query_type']],\n",
    "        alpha=0.7, edgecolors='black', linewidths=0.5\n",
    "    )\n",
    "    ax.annotate(\n",
    "        row['query_id'],\n",
    "        (row['winner_gap'], row['score_spread']),\n",
    "        textcoords='offset points', xytext=(6, 4), fontsize=8\n",
    "    )\n",
    "\n",
    "# Legend for query type\n",
    "handles = [plt.Line2D([0], [0], marker='o', color='w',\n",
    "           markerfacecolor=c, markersize=9, label=t)\n",
    "           for t, c in colour.items()]\n",
    "ax.legend(handles=handles, title='Query type', fontsize=8, loc='upper left')\n",
    "\n",
    "ax.set_xlabel('Winner Gap  (score[1] âˆ’ score[2])', fontsize=10)\n",
    "ax.set_ylabel('Score Spread  (score[1] âˆ’ score[10])', fontsize=10)\n",
    "ax.set_title('Retrieval Confidence: Gap vs. Spread\\n'\n",
    "             '(bubble size = top-1 score)', fontsize=11)\n",
    "ax.axvline(0, color='grey', linewidth=0.5)\n",
    "ax.axhline(0, color='grey', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.savefig('exercise9_gap_vs_spread.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('âœ” Saved: exercise9_gap_vs_spread.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Plot 3: Summary table â€” top scores + gap + std + chunks above threshold â”€â”€\n",
    "print('=== SCORE SUMMARY TABLE ===')\n",
    "pd.set_option('display.max_colwidth', 55)\n",
    "display(df_summary[[\n",
    "    'query_id', 'query_type', 'top1_score', 'top2_score',\n",
    "    'winner_gap', 'score_std', 'n_above_threshold'\n",
    "]].sort_values('winner_gap', ascending=False))\n",
    "\n",
    "print(f'\\nThreshold used: {THRESHOLD}')\n",
    "print(f'Queries with clear winner (gap > 0.05): '\n",
    "      f\"{(df_summary['winner_gap'] > 0.05).sum()}\")\n",
    "print(f'Queries with tight cluster  (std < 0.02): '\n",
    "      f\"{(df_summary['score_std'] < 0.02).sum()}\")\n",
    "print(f'Queries with 0 chunks above threshold: '\n",
    "      f\"{(df_summary['n_above_threshold'] == 0).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Plot 4: Threshold sensitivity â€” how many chunks survive at different thresholds? â”€â”€\n",
    "thresholds = np.arange(0.1, 0.8, 0.05)\n",
    "threshold_data = []\n",
    "\n",
    "for t in thresholds:\n",
    "    n_pass = (df_scores['score'] >= t).sum()\n",
    "    q_pass = df_scores[df_scores['score'] >= t]['query_id'].nunique()\n",
    "    threshold_data.append({'threshold': round(t, 2),\n",
    "                            'chunks_above': n_pass,\n",
    "                            'queries_with_any_chunk': q_pass})\n",
    "\n",
    "df_thresh = pd.DataFrame(threshold_data)\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(9, 4))\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax1.bar(df_thresh['threshold'], df_thresh['chunks_above'],\n",
    "        width=0.04, alpha=0.6, color='steelblue', label='Chunks above threshold')\n",
    "ax2.plot(df_thresh['threshold'], df_thresh['queries_with_any_chunk'],\n",
    "         color='crimson', marker='o', linewidth=2, label='Queries with â‰¥1 chunk')\n",
    "\n",
    "ax1.axvline(THRESHOLD, color='black', linestyle='--', linewidth=1,\n",
    "            label=f'Current threshold ({THRESHOLD})')\n",
    "\n",
    "ax1.set_xlabel('Score Threshold', fontsize=10)\n",
    "ax1.set_ylabel('Chunks Surviving', fontsize=10, color='steelblue')\n",
    "ax2.set_ylabel('Queries with â‰¥1 Chunk', fontsize=10, color='crimson')\n",
    "ax2.set_ylim(0, len(QUERIES) + 1)\n",
    "ax2.yaxis.set_major_locator(mticker.MaxNLocator(integer=True))\n",
    "\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, fontsize=8, loc='upper right')\n",
    "\n",
    "plt.title('Threshold Sensitivity: How Many Chunks / Queries Survive?', fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.savefig('exercise9_threshold_sensitivity.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('âœ” Saved: exercise9_threshold_sensitivity.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Answer Comparison: No Threshold vs. Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 300)\n",
    "\n",
    "for _, row in df_summary.iterrows():\n",
    "    print(f\"\\n{'='*65}\")\n",
    "    print(f\"[{row['query_id']}] {row['question']}\")\n",
    "    print(f\"  top1={row['top1_score']}  gap={row['winner_gap']}  \"\n",
    "          f\"chunks_above_threshold={row['n_above_threshold']}\")\n",
    "    print(f\"\\n  NO THRESHOLD ({row['chunks_no_thresh']} chunks):\")\n",
    "    print(f\"  {row['answer_no_thresh'][:400]}\")\n",
    "    print(f\"\\n  THRESHOLD â‰¥ {THRESHOLD} ({row['chunks_with_thresh']} chunks):\")\n",
    "    print(f\"  {row['answer_with_thresh'][:400]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Documentation Questions\n",
    "\n",
    "Fill in your observations after reviewing the outputs and charts above.\n",
    "\n",
    "### 1. When is there a clear \"winner\" (large gap between #1 and #2)?\n",
    "*Look at the `winner_gap` column and the score profile charts. Which query types tend to produce large gaps? What does a large gap tell you about the queryâ€“corpus match?*\n",
    "\n",
    "---\n",
    "\n",
    "### 2. When are scores tightly clustered (ambiguous retrieval)?\n",
    "*Look at `score_std`. Which queries had the lowest spread? Were these off-topic queries, broad questions, or something else? What happens to answer quality when retrieval is ambiguous?*\n",
    "\n",
    "---\n",
    "\n",
    "### 3. What score threshold would you use to filter out irrelevant results?\n",
    "*Look at the threshold sensitivity chart. At what value does the number of surviving queries start dropping sharply? Compare answer quality at your chosen threshold vs. no threshold for the off-topic query (Q09).*\n",
    "\n",
    "---\n",
    "\n",
    "### 4. How does score distribution correlate with answer quality?\n",
    "*For queries where `winner_gap` is large, is the answer better? For queries where all scores are low (e.g. Q09 off-topic), does the model hallucinate or correctly say it doesn't know?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all output files in Colab\n",
    "output_files = [\n",
    "    'exercise9_score_details.csv',\n",
    "    'exercise9_score_summary.csv',\n",
    "    'exercise9_score_profiles.png',\n",
    "    'exercise9_gap_vs_spread.png',\n",
    "    'exercise9_threshold_sensitivity.png',\n",
    "]\n",
    "try:\n",
    "    from google.colab import files as colab_files\n",
    "    for fname in output_files:\n",
    "        if os.path.exists(fname):\n",
    "            colab_files.download(fname)\n",
    "            print(f' Downloading: {fname}')\n",
    "        else:\n",
    "            print(f' Not found (skipping): {fname}')\n",
    "except ImportError:\n",
    "    print('Files saved locally:')\n",
    "    for fname in output_files:\n",
    "        print(f'  {fname}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
