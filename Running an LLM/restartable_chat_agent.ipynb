{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNTLYCNcRMwtp4ZMDWEG1++"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"id":"gD_VPGvUqyK1","executionInfo":{"status":"ok","timestamp":1769037296067,"user_tz":300,"elapsed":2285,"user":{"displayName":"Nadia","userId":"17436501635847667941"}}},"outputs":[],"source":["from google.colab import userdata\n","hf_token = userdata.get('HF_TOKEN')\n","\n","# Use it with huggingface_hub\n","from huggingface_hub import login\n","login(token=hf_token)"]},{"cell_type":"code","source":["\"\"\"\n","Enhanced Chat Agent for Llama 3.2-1B-Instruct\n","Token Budget Context Management with Pickle-based Restart Capability\n","\n","This implementation includes:\n","✓ Token-based budgeting context management\n","✓ Pickle state persistence (NEW!)\n","✓ Restartable after interruption (NEW!)\n","✓ History toggle (ON/OFF)\n","✓ All 4 test cases covered\n","\"\"\"\n","\n","import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","import pickle\n","import os\n","import sys\n","import signal\n","from datetime import datetime\n","from typing import List, Dict, Optional\n","\n","# ============================================================================\n","# CONFIGURATION\n","# ============================================================================\n","\n","MODEL_NAME = \"meta-llama/Llama-3.2-1B-Instruct\"\n","SYSTEM_PROMPT = \"You are a helpful AI assistant. Be concise and friendly.\"\n","STATE_FILE = \"llama_chat_state.pkl\"  # Pickle file for state persistence\n","\n","# ============================================================================\n","# BEST PRACTICE 2: SET APPROPRIATE MAX LENGTH\n","# ============================================================================\n","# From professor's guide: Leave room for response generation\n","MAX_CONTEXT_LENGTH = 2048  # Total context window for Llama 3.2-1B\n","MAX_NEW_TOKENS = 512       # Maximum tokens for response\n","# Effective history limit: 2048 - 512 = 1536 tokens\n","MAX_CONTEXT_TOKENS = MAX_CONTEXT_LENGTH - MAX_NEW_TOKENS\n","\n","# History toggle\n","USE_CONVERSATION_HISTORY = True  # Set to False to disable memory\n","\n","# ============================================================================\n","# RESTARTABLE CHAT STATE CLASS\n","# ============================================================================\n","\n","class RestartableChatState:\n","    \"\"\"\n","    Manages chat state with pickle persistence for restart capability.\n","    \"\"\"\n","\n","    def __init__(self, state_file: str = STATE_FILE):\n","        self.state_file = state_file\n","\n","        # State variables to persist\n","        self.full_chat_history: List[Dict] = []\n","        self.working_chat_history: List[Dict] = []\n","        self.turn_number = 0\n","        self.session_start_time: Optional[datetime] = None\n","        self.total_restarts = 0\n","        self.last_save_time: Optional[datetime] = None\n","\n","        # Configuration to persist\n","        self.max_context_tokens = MAX_CONTEXT_TOKENS\n","        self.max_new_tokens = MAX_NEW_TOKENS\n","        self.use_conversation_history = USE_CONVERSATION_HISTORY\n","\n","        # Set up signal handlers for graceful shutdown\n","        signal.signal(signal.SIGINT, self._signal_handler)\n","        signal.signal(signal.SIGTERM, self._signal_handler)\n","\n","        # Try to load existing state\n","        self._load_state()\n","\n","    def _signal_handler(self, signum, frame):\n","        \"\"\"Handle termination signals gracefully.\"\"\"\n","        print(f\"\\n\\n Received signal {signum}. Saving state before exit...\")\n","        self._save_state()\n","        print(\" State saved successfully. Safe to exit.\")\n","        print(f\" State file: {self.state_file}\")\n","        sys.exit(0)\n","\n","    def _save_state(self):\n","        \"\"\"Save current state to pickle file.\"\"\"\n","        state = {\n","            'full_chat_history': self.full_chat_history,\n","            'working_chat_history': self.working_chat_history,\n","            'turn_number': self.turn_number,\n","            'session_start_time': self.session_start_time,\n","            'total_restarts': self.total_restarts,\n","            'last_save_time': datetime.now(),\n","            'max_context_tokens': self.max_context_tokens,\n","            'max_new_tokens': self.max_new_tokens,\n","            'use_conversation_history': self.use_conversation_history\n","        }\n","\n","        try:\n","            # Atomic write: temp file + rename\n","            temp_file = f\"{self.state_file}.tmp\"\n","            with open(temp_file, 'wb') as f:\n","                pickle.dump(state, f, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","            os.replace(temp_file, self.state_file)\n","            self.last_save_time = state['last_save_time']\n","\n","        except Exception as e:\n","            print(f\" Error saving state: {e}\")\n","            if os.path.exists(temp_file):\n","                os.remove(temp_file)\n","\n","    def _load_state(self):\n","        \"\"\"Load state from pickle file if it exists.\"\"\"\n","        if not os.path.exists(self.state_file):\n","            print(\" No previous state found. Starting fresh session.\")\n","            self.session_start_time = datetime.now()\n","\n","            # Initialize with system message\n","            system_message = {\"role\": \"system\", \"content\": SYSTEM_PROMPT}\n","            self.full_chat_history.append(system_message)\n","            self.working_chat_history.append(system_message)\n","            return\n","\n","        try:\n","            with open(self.state_file, 'rb') as f:\n","                state = pickle.load(f)\n","\n","            # Restore all state\n","            self.full_chat_history = state.get('full_chat_history', [])\n","            self.working_chat_history = state.get('working_chat_history', [])\n","            self.turn_number = state.get('turn_number', 0)\n","            self.session_start_time = state.get('session_start_time', datetime.now())\n","            self.total_restarts = state.get('total_restarts', 0) + 1\n","            self.last_save_time = state.get('last_save_time')\n","\n","            # Restore configuration\n","            self.max_context_tokens = state.get('max_context_tokens', MAX_CONTEXT_TOKENS)\n","            self.max_new_tokens = state.get('max_new_tokens', MAX_NEW_TOKENS)\n","            self.use_conversation_history = state.get('use_conversation_history', USE_CONVERSATION_HISTORY)\n","\n","            print(\"\\n\" + \"=\"*70)\n","            print(\" STATE RESTORED SUCCESSFULLY!\")\n","            print(\"=\"*70)\n","            print(f\"  Restart #{self.total_restarts}\")\n","            print(f\"    {self.turn_number} turns completed\")\n","            print(f\"    {len(self.full_chat_history)} messages in full history\")\n","            print(f\"    {len(self.working_chat_history)} messages in working memory\")\n","            print(f\"    Session started: {self.session_start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n","            if self.last_save_time:\n","                print(f\"    Last saved: {self.last_save_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n","            print(\"=\"*70 + \"\\n\")\n","\n","        except Exception as e:\n","            print(f\" Error loading state: {e}\")\n","            print(\" Starting with fresh state.\")\n","            self.session_start_time = datetime.now()\n","\n","            # Initialize with system message\n","            system_message = {\"role\": \"system\", \"content\": SYSTEM_PROMPT}\n","            self.full_chat_history.append(system_message)\n","            self.working_chat_history.append(system_message)\n","\n","    def save(self):\n","        \"\"\"Public method to save state.\"\"\"\n","        self._save_state()\n","\n","    def clear_state(self):\n","        \"\"\"Clear all state and delete the state file.\"\"\"\n","        self.full_chat_history = []\n","        self.working_chat_history = []\n","        self.turn_number = 0\n","        self.session_start_time = datetime.now()\n","        self.total_restarts = 0\n","        self.last_save_time = None\n","\n","        # Re-initialize with system message\n","        system_message = {\"role\": \"system\", \"content\": SYSTEM_PROMPT}\n","        self.full_chat_history.append(system_message)\n","        self.working_chat_history.append(system_message)\n","\n","        if os.path.exists(self.state_file):\n","            os.remove(self.state_file)\n","            print(\"  State cleared and file deleted.\")\n","\n","    def get_stats_dict(self):\n","        \"\"\"Get statistics about the current state.\"\"\"\n","        return {\n","            'turn_number': self.turn_number,\n","            'total_messages': len(self.full_chat_history),\n","            'working_messages': len(self.working_chat_history),\n","            'total_restarts': self.total_restarts,\n","            'session_start': self.session_start_time,\n","            'last_save': self.last_save_time\n","        }\n","\n","\n","# ============================================================================\n","# LOAD MODEL\n","# ============================================================================\n","\n","print(\"Loading model (this takes 1-2 minutes)...\")\n","\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    MODEL_NAME,\n","    torch_dtype=torch.float16,\n","    device_map=\"auto\",\n","    low_cpu_mem_usage=True\n",")\n","\n","model.eval()\n","print(f\" Model loaded! Using device: {model.device}\")\n","print(f\" Memory usage: ~2.5 GB (FP16)\\n\")\n","\n","# ============================================================================\n","# INITIALIZE RESTARTABLE STATE\n","# ============================================================================\n","\n","chat_state = RestartableChatState(STATE_FILE)\n","\n","# ============================================================================\n","# DISPLAY CONFIGURATION\n","# ============================================================================\n","print(\"=\"*70)\n","print(\"CONFIGURATION:\")\n","print(f\"  • Conversation History: {'ENABLED' if chat_state.use_conversation_history else 'DISABLED'}\")\n","print(f\"  • Context Strategy: TOKEN_BUDGET\")\n","print(f\"  • Max Context Length: {MAX_CONTEXT_LENGTH} tokens (total window)\")\n","print(f\"  • Max Response Tokens: {chat_state.max_new_tokens} tokens\")\n","print(f\"  • Effective History Limit: {chat_state.max_context_tokens} tokens\")\n","print(f\"  • State Persistence: ENABLED (pickle)\")\n","print(f\"  • State File: {STATE_FILE}\")\n","print(\"=\"*70 + \"\\n\")\n","\n","# ============================================================================\n","# BEST PRACTICE 6: TOKEN COUNTING (FAST AND ACCURATE)\n","# ============================================================================\n","\n","def approximate_token_count(text):\n","    \"\"\"\n","    Fast token count approximation.\n","    From professor's guide: approx_tokens = len(text.split()) * 1.3\n","    \"\"\"\n","    return int(len(text.split()) * 1.3)\n","\n","\n","def accurate_token_count(messages):\n","    \"\"\"\n","    Accurate token count using actual tokenization.\n","    From professor's guide: actual_tokens = len(tokenizer.encode(text))\n","    \"\"\"\n","    try:\n","        formatted = tokenizer.apply_chat_template(\n","            messages,\n","            add_generation_prompt=True,\n","            tokenize=False\n","        )\n","        tokens = tokenizer.encode(formatted)\n","        return len(tokens)\n","    except:\n","        # Fallback to approximation\n","        total = 0\n","        for msg in messages:\n","            total += approximate_token_count(msg[\"content\"])\n","        return total\n","\n","\n","# ============================================================================\n","# BEST PRACTICE 1: MONITOR TOKEN USAGE\n","# ============================================================================\n","\n","def get_context_stats(history):\n","    \"\"\"\n","    Get statistics about current context usage.\n","    From professor's guide: stats = chat_manager.get_context_stats()\n","    \"\"\"\n","    num_tokens = accurate_token_count(history)\n","    num_messages = len([msg for msg in history if msg[\"role\"] != \"system\"])\n","\n","    return {\n","        'num_tokens': num_tokens,\n","        'max_tokens': chat_state.max_context_tokens,\n","        'num_messages': num_messages,\n","        'utilization': f\"{(num_tokens/chat_state.max_context_tokens)*100:.1f}%\",\n","        'tokens_remaining': chat_state.max_context_tokens - num_tokens\n","    }\n","\n","\n","# ============================================================================\n","# TOKEN BUDGET CONTEXT MANAGEMENT\n","# ============================================================================\n","\n","def token_budget_management(history, max_tokens):\n","    \"\"\"\n","    Token-based context management with all best practices.\n","\n","    Implements:\n","    - BEST PRACTICE 3: Preserve system prompts\n","    - BEST PRACTICE 4: Edge case handling\n","    - BEST PRACTICE 4: User warnings when truncating\n","    \"\"\"\n","    # BEST PRACTICE 3: Always preserve system message\n","    system_msgs = [msg for msg in history if msg[\"role\"] == \"system\"]\n","    conversation_msgs = [msg for msg in history if msg[\"role\"] != \"system\"]\n","\n","    # BEST PRACTICE 4: Handle edge case - prevent infinite loops\n","    if len(conversation_msgs) <= 1:\n","        return history  # Can't truncate further\n","\n","    # Start with all messages\n","    current_history = system_msgs + conversation_msgs\n","    current_tokens = accurate_token_count(current_history)\n","\n","    # Remove oldest messages until under budget\n","    removed_count = 0\n","    while current_tokens > max_tokens and len(conversation_msgs) > 2:\n","        # Remove oldest message pair (user + assistant)\n","        conversation_msgs = conversation_msgs[2:]\n","        current_history = system_msgs + conversation_msgs\n","        current_tokens = accurate_token_count(current_history)\n","        removed_count += 2\n","\n","    # BEST PRACTICE 4: Warn user when truncating\n","    if removed_count > 0:\n","        print(f\"  [Context management: Removed {removed_count} old messages]\")\n","        print(f\"   Current context: {current_tokens}/{max_tokens} tokens\")\n","\n","    return current_history\n","\n","\n","# ============================================================================\n","# MAIN CHAT LOOP\n","# ============================================================================\n","\n","print(\"Chat started! Type 'quit' or 'exit' to end the conversation.\")\n","print(\"Special commands:\")\n","print(\"  • 'stats'   - Show detailed context statistics\")\n","print(\"  • 'history' - Show conversation history\")\n","print(\"  • 'clear'   - Clear all state and start fresh\")\n","print(\"  • 'save'    - Manually save current state\")\n","print(\"  • Ctrl+C    - Interrupt (state auto-saved)\")\n","print()\n","print(\"=\"*70 + \"\\n\")\n","\n","while True:\n","    # ========================================================================\n","    # Get user input\n","    # ========================================================================\n","    try:\n","        user_input = input(\"You: \").strip()\n","    except (KeyboardInterrupt, EOFError):\n","        print(\"\\n\\n Interrupted. Saving state...\")\n","        chat_state.save()\n","        print(\" State saved. Safe to exit.\")\n","        break\n","\n","    # Handle special commands\n","    if user_input.lower() in ['quit', 'exit', 'q']:\n","        print(\"\\n\" + \"=\"*70)\n","        print(\"CONVERSATION SUMMARY:\")\n","        stats = chat_state.get_stats_dict()\n","        print(f\"  • Total turns: {stats['turn_number']}\")\n","        print(f\"  • Total messages: {stats['total_messages']}\")\n","        print(f\"  • Total restarts: {stats['total_restarts']}\")\n","        if chat_state.use_conversation_history:\n","            ctx_stats = get_context_stats(chat_state.working_chat_history)\n","            print(f\"  • Final tokens: {ctx_stats['num_tokens']}/{ctx_stats['max_tokens']}\")\n","            print(f\"  • Context utilization: {ctx_stats['utilization']}\")\n","        print(\"=\"*70)\n","\n","        # Save state before exit\n","        print(\"\\n Saving state before exit...\")\n","        chat_state.save()\n","        print(\" State saved successfully!\")\n","        print(f\" State file: {STATE_FILE}\")\n","        print(\"\\nGoodbye! Run again to continue where you left off.\")\n","        break\n","\n","    # Show statistics\n","    if user_input.lower() == 'stats':\n","        ctx_stats = get_context_stats(chat_state.working_chat_history)\n","        state_stats = chat_state.get_stats_dict()\n","\n","        print(\"\\n\" + \"=\"*70)\n","        print(\"CONTEXT STATISTICS:\")\n","        print(f\"  • Tokens used: {ctx_stats['num_tokens']}/{ctx_stats['max_tokens']}\")\n","        print(f\"  • Utilization: {ctx_stats['utilization']}\")\n","        print(f\"  • Messages in context: {ctx_stats['num_messages']}\")\n","        print(f\"  • Tokens remaining: {ctx_stats['tokens_remaining']}\")\n","        print(\"\\nSESSION STATISTICS:\")\n","        print(f\"  • Turn number: {state_stats['turn_number']}\")\n","        print(f\"  • Total messages: {state_stats['total_messages']}\")\n","        print(f\"  • Working messages: {state_stats['working_messages']}\")\n","        print(f\"  • Total restarts: {state_stats['total_restarts']}\")\n","        print(f\"  • Session start: {state_stats['session_start'].strftime('%Y-%m-%d %H:%M:%S')}\")\n","        if state_stats['last_save']:\n","            print(f\"  • Last saved: {state_stats['last_save'].strftime('%Y-%m-%d %H:%M:%S')}\")\n","        print(\"=\"*70 + \"\\n\")\n","        continue\n","\n","    # Show history\n","    if user_input.lower() == 'history':\n","        print(\"\\n\" + \"=\"*70)\n","        print(\"CONVERSATION HISTORY:\")\n","        print(\"=\"*70)\n","        for i, msg in enumerate(chat_state.full_chat_history, 1):\n","            role = msg['role'].upper()\n","            content = msg['content'][:100]\n","            if len(msg['content']) > 100:\n","                content += \"...\"\n","            print(f\"{i}. [{role}] {content}\")\n","        print(\"=\"*70 + \"\\n\")\n","        continue\n","\n","    # Clear state\n","    if user_input.lower() == 'clear':\n","        confirm = input(\"  Clear all state and start fresh? (yes/no): \")\n","        if confirm.lower() == 'yes':\n","            chat_state.clear_state()\n","            print(\" State cleared. Starting fresh conversation.\\n\")\n","        continue\n","\n","    # Manual save\n","    if user_input.lower() == 'save':\n","        chat_state.save()\n","        print(\" State saved manually.\\n\")\n","        continue\n","\n","    if not user_input:\n","        continue\n","\n","    chat_state.turn_number += 1\n","\n","    # ========================================================================\n","    # Add user message to histories\n","    # ========================================================================\n","    user_message = {\"role\": \"user\", \"content\": user_input}\n","    chat_state.full_chat_history.append(user_message)\n","\n","    # ========================================================================\n","    # Prepare working history based on settings\n","    # ========================================================================\n","    if chat_state.use_conversation_history:\n","        # Add to working history\n","        chat_state.working_chat_history.append(user_message)\n","\n","        # Apply token budget context management\n","        managed_history = token_budget_management(\n","            chat_state.working_chat_history,\n","            chat_state.max_context_tokens\n","        )\n","\n","        # Update working history with managed version\n","        chat_state.working_chat_history = managed_history\n","\n","    else:\n","        # NO HISTORY MODE: Only system + current message\n","        system_message = {\"role\": \"system\", \"content\": SYSTEM_PROMPT}\n","        managed_history = [system_message, user_message]\n","        print(f\" [No history mode: Bot sees only current message]\")\n","\n","    # ========================================================================\n","    # BEST PRACTICE 1: MONITOR TOKEN USAGE\n","    # ========================================================================\n","    stats = get_context_stats(managed_history)\n","    print(f\" [Context: {stats['num_tokens']}/{stats['max_tokens']} tokens \" +\n","          f\"({stats['utilization']} used) | {stats['num_messages']} messages]\")\n","\n","    # ========================================================================\n","    # Tokenize\n","    # ========================================================================\n","    input_ids = tokenizer.apply_chat_template(\n","        managed_history,\n","        add_generation_prompt=True,\n","        return_tensors=\"pt\"\n","    ).to(model.device)\n","\n","    attention_mask = torch.ones_like(input_ids)\n","\n","    # ========================================================================\n","    # Generate response\n","    # ========================================================================\n","    print(\"Assistant: \", end=\"\", flush=True)\n","\n","    with torch.no_grad():\n","        outputs = model.generate(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            max_new_tokens=chat_state.max_new_tokens,  # BEST PRACTICE 2\n","            do_sample=True,\n","            temperature=0.7,\n","            top_p=0.9,\n","            pad_token_id=tokenizer.eos_token_id\n","        )\n","\n","    # ========================================================================\n","    # Decode response\n","    # ========================================================================\n","    new_tokens = outputs[0][input_ids.shape[1]:]\n","    assistant_response = tokenizer.decode(\n","        new_tokens,\n","        skip_special_tokens=True\n","    )\n","\n","    print(assistant_response)\n","\n","    # ========================================================================\n","    # Add assistant response to histories\n","    # ========================================================================\n","    assistant_message = {\"role\": \"assistant\", \"content\": assistant_response}\n","    chat_state.full_chat_history.append(assistant_message)\n","\n","    if chat_state.use_conversation_history:\n","        chat_state.working_chat_history.append(assistant_message)\n","\n","    # ========================================================================\n","    # AUTO-SAVE STATE AFTER EACH TURN\n","    # ========================================================================\n","    chat_state.save()\n","\n","    print()\n","\n","# ============================================================================\n","# IMPLEMENTATION SUMMARY\n","# ============================================================================\n","\"\"\"\n","ENHANCED FEATURES:\n","\n","✓ PICKLE STATE PERSISTENCE\n","  - Automatic save after each turn\n","  - Graceful handling of Ctrl+C and kill signals\n","  - Atomic file writes prevent corruption\n","  - State restored on restart\n","\n","✓ RESTART CAPABILITY\n","  - Tracks restart count\n","  - Preserves full conversation history\n","  - Maintains token budget state\n","  - Continues from exact point of interruption\n","\n","CONTEXT MANAGEMENT STRATEGY: Token Budget\n","\n","BEST PRACTICES IMPLEMENTED:\n","\n","✓ 1. Monitor Token Usage\n","✓ 2. Set Appropriate Max Length\n","✓ 3. Preserve System Prompts\n","✓ 4. Handle Edge Cases\n","✓ 5. Optimize for Use Case\n","✓ 6. Token Counting\n","\n","TEST CASES COVERED:\n","\n","✓ Test Case 1: Context Overflow\n","✓ Test Case 2: System Prompt Preservation\n","✓ Test Case 3: Token Counting Accuracy\n","✓ Test Case 4: Multi-turn Coherence\n","✓ Test Case 5: Restart After Interruption (NEW!)\n","\n","USAGE:\n","\n","1. Normal conversation - state auto-saved after each turn\n","2. Ctrl+C or kill - state saved before exit\n","3. Restart script - conversation continues where it left off\n","4. Type 'clear' to start fresh\n","5. Type 'save' to manually save state\n","\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"DwVv6Voxq65g","executionInfo":{"status":"error","timestamp":1769038666681,"user_tz":300,"elapsed":371615,"user":{"displayName":"Nadia","userId":"17436501635847667941"}},"outputId":"6e41399c-ea2e-40c3-b1c1-b7270bc8f8df"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading model (this takes 1-2 minutes)...\n"," Model loaded! Using device: cpu\n"," Memory usage: ~2.5 GB (FP16)\n","\n","\n","======================================================================\n"," STATE RESTORED SUCCESSFULLY!\n","======================================================================\n","  Restart #1\n","    4 turns completed\n","    9 messages in full history\n","    9 messages in working memory\n","    Session started: 2026-01-21 23:16:01\n","    Last saved: 2026-01-21 23:31:19\n","======================================================================\n","\n","======================================================================\n","CONFIGURATION:\n","  • Conversation History: ENABLED\n","  • Context Strategy: TOKEN_BUDGET\n","  • Max Context Length: 2048 tokens (total window)\n","  • Max Response Tokens: 512 tokens\n","  • Effective History Limit: 1536 tokens\n","  • State Persistence: ENABLED (pickle)\n","  • State File: llama_chat_state.pkl\n","======================================================================\n","\n","Chat started! Type 'quit' or 'exit' to end the conversation.\n","Special commands:\n","  • 'stats'   - Show detailed context statistics\n","  • 'history' - Show conversation history\n","  • 'clear'   - Clear all state and start fresh\n","  • 'save'    - Manually save current state\n","  • Ctrl+C    - Interrupt (state auto-saved)\n","\n","======================================================================\n","\n","You: What was I struggling with as a student\n"," [Context: 558/1536 tokens (36.3% used) | 9 messages]\n","Assistant: As a 1st-year PhD student in Computer Science at UVA, I think you might be struggling with managing your time and prioritizing tasks.\n","\n","One specific challenge you might be facing is balancing your research and coursework responsibilities. Many PhD students struggle to find a balance between the two, and it can be overwhelming to keep up with the demands of both.\n","\n","Additionally, you might be feeling the pressure to produce high-quality research and coursework, while also meeting the expectations of your advisors and department.\n","\n","It's also possible that you're feeling overwhelmed by the amount of information being thrown at you, and struggling to prioritize what's truly important.\n","\n","Remember, it's normal to feel this way, and it doesn't mean you're not capable or competent. It's just a sign that you need to make some adjustments to your approach.\n","\n","Would you like to talk more about what's specifically challenging for you, Nadia?\n","\n","\n","\n"," Received signal 2. Saving state before exit...\n"," State saved successfully. Safe to exit.\n"," State file: llama_chat_state.pkl\n"]},{"output_type":"error","ename":"SystemExit","evalue":"0","traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n","  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"]}]}]}